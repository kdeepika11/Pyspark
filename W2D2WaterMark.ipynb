{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f56fa36b-bc03-440e-8a00-7bef9b01b8e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "create table if not exists workspace.default.etl_watermarks(\n",
    "pipeline string,\n",
    "entity string,\n",
    "watermark_ts timestamp\n",
    ")       \n",
    "using delta   \n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "INSERT into workspace.default.etl_watermarks \n",
    "SELECT 'demo_pipeline', 'orders', TIMESTAMP('1900-01-01 00:00:00')\n",
    "WHERE NOT EXISTS (\n",
    "  SELECT 1 FROM workspace.default.etl_watermarks\n",
    "  WHERE pipeline='demo_pipeline' AND entity='orders'\n",
    ")\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "display(spark.table(\"workspace.default.etl_watermarks\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d7d6e9ae-48af-45b1-8d3a-830dec160704",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "wm = spark.sql(\"\"\"\n",
    "SELECT watermark_ts\n",
    "FROM workspace.default.etl_watermarks\n",
    "WHERE pipeline='demo_pipeline' AND entity='orders'\n",
    "\"\"\").first()[0]\n",
    "\n",
    "print(\"Current watermark:\", wm)\n",
    "new_orders_data = [\n",
    "    (1, 107, 155.0, \"2025-12-08\"),  # update existing order\n",
    "    (2, 110,  95.0, \"2025-12-13\"),  # new order\n",
    "    (6, 111,  40.0, \"2025-12-14\")   # new customer order\n",
    "]\n",
    "\n",
    "incoming = (spark.createDataFrame(new_orders_data, [\"CustomerId\",\"OrderId\",\"Amount\",\"OrderDate\"])\n",
    "            .withColumn(\"OrderDate\", F.to_date(\"OrderDate\"))\n",
    "            .withColumn(\"_ingested_at\", F.current_timestamp())\n",
    "            .withColumn(\"_source\", F.lit(\"day9_batch\"))\n",
    ")\n",
    "display(incoming)\n",
    "\n",
    "incoming.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"workspace.default.bronze_orders\")\n",
    "\n",
    "bronze_orders= spark.table(\"workspace.default.bronze_orders\")\n",
    "bronze_new= bronze_orders.filter(F.col(\"_ingested_at\") > wm)\n",
    "display(bronze_new)\n",
    "\n",
    "w = Window.partitionBy(\"CustomerId\",\"OrderId\").orderBy(F.col(\"_ingested_at\").desc())\n",
    "silver_incr = (bronze_new\n",
    "  .withColumn(\"CustomerId\", F.col(\"CustomerId\").cast(\"long\"))\n",
    "  .withColumn(\"OrderId\", F.col(\"OrderId\").cast(\"long\"))\n",
    "  .withColumn(\"Amount\", F.col(\"Amount\").cast(\"double\"))\n",
    "  .withColumn(\"OrderDate\", F.col(\"OrderDate\").cast(\"date\"))\n",
    "  .withColumn(\"UpdatedAt\", F.current_timestamp())\n",
    "  .withColumn(\"rn\", F.row_number().over(w))\n",
    "  .filter(F.col(\"rn\") == 1)\n",
    "  .drop(\"rn\")\n",
    ")\n",
    "\n",
    "display(silver_incr)\n",
    "silver_incr.createOrReplaceTempView(\"src\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO workspace.default.silver_orders AS tgt\n",
    "USING src\n",
    "ON tgt.CustomerId = src.CustomerId AND tgt.OrderId = src.OrderId\n",
    "WHEN MATCHED THEN UPDATE SET\n",
    "  tgt.Amount = src.Amount,\n",
    "  tgt.OrderDate = src.OrderDate,\n",
    "  tgt._ingested_at = src._ingested_at,\n",
    "  tgt._source = src._source\n",
    "WHEN NOT MATCHED THEN INSERT *\n",
    "\"\"\")\n",
    "\n",
    "display(spark.table(\"workspace.default.silver_orders\").orderBy(\"CustomerId\",\"OrderDate\",\"OrderId\"))\n",
    "\n",
    "new_wm = bronze_new.agg(F.max(\"_ingested_at\").alias(\"max_ts\")).first()[\"max_ts\"]\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "UPDATE workspace.default.etl_watermarks\n",
    "SET watermark_ts = TIMESTAMP('{new_wm}')\n",
    "WHERE pipeline='demo_pipeline' AND entity='orders'\n",
    "\"\"\")\n",
    "\n",
    "print(\"Updated watermark to:\", new_wm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbedb534-3e12-4882-92f3-1e50f47c3d12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "W2D2WaterMark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
