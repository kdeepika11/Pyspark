{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a696335a-6276-45e7-b207-fbb1206959e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "customer= spark.table(\"workspace.default.customers\")\n",
    "display(customer)\n",
    "\n",
    "orders_data = [\n",
    "    (101, 1, 120.0, \"2025-12-01\"),\n",
    "    (102, 1,  75.0, \"2025-12-02\"),\n",
    "    (103, 2,  50.0, \"2025-12-03\"),\n",
    "    (104, 4, 200.0, \"2025-12-05\"),\n",
    "    (106, 4, 210.0, \"2025-12-07\"),  # newer order for customer 4\n",
    "    (105, 99, 60.0, \"2025-12-06\")\n",
    "]\n",
    "orders = (spark.createDataFrame(orders_data, [\"OrderId\", \"CustomerId\", \"Amount\", \"OrderDate\"])\n",
    "          .withColumn(\"OrderDate\", F.to_date(\"OrderDate\"))\n",
    ")\n",
    "display(orders)\n",
    "#out = \"/Volumes/workspace/default/mainvol/deepika/week1/orders_parquet\"\n",
    "#orders.write.mode(\"overwrite\").parquet(out)\n",
    "#display(spark.read.parquet(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "222dbdfd-9478-4ed4-addd-aa01b0f9a709",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "w=Window.partitionBy(F.col(\"CustomerId\")).orderBy(F.col(\"OrderDate\").desc())\n",
    "ranked= orders.withColumn(\"rn\",F.row_number().over(w))\n",
    "latest= ranked.filter(F.col(\"rn\")==1)\n",
    "display(latest)\n",
    "\n",
    "cust_latest=(customer\n",
    ".join(latest,on=\"CustomerId\",how=\"left\")\n",
    ".select(\"CustomerId\", \"CustomerName\", \"Country\", \"OrderId\", \"Amount\", \"OrderDate\")\n",
    ".orderBy(\"CustomerId\")\n",
    ")\n",
    "display(cust_latest)\n",
    "\n",
    "big_latest = cust_latest.filter(F.col(\"Amount\") >= 100)\n",
    "display(big_latest.orderBy(F.col(\"Amount\").desc()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "247b1540-6433-4a9d-b7ee-1c818efae118",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766516845030}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "orders_hist_data = [\n",
    "    (1, 101, 120.0, \"2025-12-01\", \"2025-12-01\"),\n",
    "    (1, 102,  75.0, \"2025-12-02\", \"2025-12-02\"),\n",
    "    (1, 103,  50.0, \"2025-12-03\", \"2025-12-03\"),\n",
    "    (1, 107, 130.0, \"2025-12-08\", \"2025-12-08\"),\n",
    "    (1, 108,  50.0, \"2025-12-10\", \"2025-12-10\"),\n",
    "    (2, 103,  50.0, \"2025-12-03\", \"2025-12-03\"),\n",
    "    (2, 108,  50.0, \"2025-12-10\", \"2025-12-10\"),\n",
    "    (4, 104, 200.0, \"2025-12-05\", \"2025-12-05\"),\n",
    "    (4, 106, 210.0, \"2025-12-07\", \"2025-12-07\")\n",
    "]\n",
    "\n",
    "orders_hist = (spark.createDataFrame(orders_hist_data, [\"CustomerId\",\"OrderId\",\"Amount\",\"OrderDate\",\"UpdatedAt\"])\n",
    "               .withColumn(\"OrderDate\", F.to_date(\"OrderDate\"))\n",
    "               .withColumn(\"UpdatedAt\", F.to_date(\"UpdatedAt\"))\n",
    ")\n",
    "display(orders_hist)\n",
    "w=Window.partitionBy(\"CustomerId\").orderBy(F.col(\"OrderDate\").desc())\n",
    "changes=(orders_hist\n",
    "         .withColumn(\"previousamount\",F.lag(\"Amount\").over(w))\n",
    "         .withColumn(\"Delta\",F.col(\"Amount\")-F.col(\"previousamount\"))\n",
    "         .withColumn(\"Ischanged\",\n",
    "                     F.when(F.col(\"previousamount\").isNull(), F.lit(0))\n",
    "                     .when(F.col(\"Amount\") != F.col(\"previousamount\"), F.lit(1))\n",
    "                     .otherwise(F.lit(0))\n",
    "         ))\n",
    "display(changes.filter(F.col(\"Ischanged\")==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ec66f7e-d0b7-46ed-8281-1a8af83b0b53",
     "showTitle": false,
     "tableResultSettingsMap": {
      "2": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"UpdatedAt\":235},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766517140400}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 2
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# orders_hist is your “initial” dataframe from Day 7\n",
    "orders_hist.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(\"workspace.default.orders_silver\")\n",
    "display(spark.table(\"workspace.default.orders_silver\"))\n",
    "\n",
    "incremental_data = [\n",
    "    (1, 107, 150.0, \"2025-12-08\"),  # updated amount (was 130)\n",
    "    (2, 109,  90.0, \"2025-12-12\")   # new order\n",
    "]\n",
    "\n",
    "incr= (spark.createDataFrame(incremental_data, [\"CustomerId\",\"OrderId\",\"Amount\",\"OrderDate\"])\n",
    "       .withColumn(\"OrderDate\", F.to_date(\"OrderDate\"))\n",
    "       .withColumn(\"UpdatedAt\",F.current_timestamp())\n",
    ")\n",
    "display(incr)\n",
    "incr.createOrReplaceTempView(\"src\")\n",
    "spark.sql(\"\"\"\n",
    "          MERGE into workspace.default.orders_silver tgt\n",
    "          using src\n",
    "          on tgt.CustomerId=src.CustomerId and tgt.OrderId=src.OrderId\n",
    "          when matched then update set tgt.Amount=src.Amount, tgt.UpdatedAt=src.UpdatedAt\n",
    "          when not matched then insert *            \n",
    "          \"\"\")\n",
    "display(spark.table(\"workspace.default.orders_silver\"))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "W2D2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
