{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dfbb180-70bf-4371-adab-e65df348ae00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import lower\n",
    "from pyspark.sql.functions import to_timestamp, lit,sum,coalesce\n",
    "from pyspark.sql.functions import col, when, lit, current_timestamp, to_date, year, month, upper, trim,count,max,countDistinct,min,desc\n",
    "\n",
    "df= spark.table(\"workspace.default.customers\")\n",
    "\n",
    "display(df)\n",
    "#************select & filters\n",
    "#df_select = df.select(\"CustomerId\", \"CustomerName\", \"Country\")\n",
    "#display(df_select)\n",
    "\n",
    "##usa = df.filter(col(\"Country\").isin(\"USA\",\"India\"))\n",
    "#display(usa)\n",
    "\n",
    "#name_has_a = df.filter(lower(col(\"CustomerName\")).contains(\"a\"  ))\n",
    "#display(name_has_a)\n",
    "\n",
    "#greaterthan3 = df.filter(col(\"CustomerId\") > 3)\n",
    "#display(greaterthan3)\n",
    "\n",
    "#afterdate= df.filter(col(\"LastUpdatedDate\") > to_timestamp(lit(\"2019-01-01\")))\n",
    "#display(afterdate)\n",
    "\n",
    "#usa_ind = df.filter((col(\"Country\").isin(\"USA\",\"Canada\")) & (col(\"CustomerId\")>2)).orderBy(col(\"LastUpdatedDate\").#desc())\n",
    "#display(usa_ind)\n",
    "#df.printSchema()\n",
    "#print(df.count())\n",
    "\n",
    "#*********** add new column , when otherwise\n",
    "df_newcolumn = df.withColumn(\"IsUSA\", when(col(\"Country\") == (\"USA\"), lit(1)).otherwise(lit(0)))\n",
    "#display(df_newcolumn)\n",
    "\n",
    "df2 = (df_newcolumn.withColumn(\"CustomerCleanName\",trim(col(\"CustomerName\")))\n",
    "       .withColumn(\"CountryUpper\",upper(trim(col(\"Country\")))))\n",
    "#display(df2)\n",
    "\n",
    "df3 = (df2\n",
    "  .withColumn(\"UpdatedDate\", to_date(col(\"LastUpdatedDate\")))\n",
    "  .withColumn(\"UpdatedYear\", year(col(\"LastUpdatedDate\")))\n",
    "  .withColumn(\"UpdatedMonth\", month(col(\"LastUpdatedDate\")))\n",
    ")\n",
    "#display(df3)\n",
    "\n",
    "df4 = df3.withColumn(\"IngestedAt\", current_timestamp())\n",
    "#display(df4)\n",
    "\n",
    "df5 = df.withColumn(\"CustomerTier\", when(col(\"Country\") == \"USA\",\"tier1\").\n",
    "                                    when(col(\"Country\") == \"Canada\",\"tier2\").otherwise(\"tier3\"))\n",
    "#display(df5)\n",
    "\n",
    "#************groupby aggregation\n",
    "by_country = df.groupBy(\"Country\").agg(count(col(\"Country\")).alias(\"numberofcustomer\")).orderBy(col(\"numberofcustomer\").desc())\n",
    "#display(by_country)\n",
    "\n",
    "latest_per_country = (df\n",
    "  .groupBy(\"Country\")\n",
    "  .agg(max(\"LastUpdatedDate\").alias(\"LatestUpdate\"))\n",
    "  .orderBy(col(\"LatestUpdate\").desc())\n",
    ")\n",
    "#display(latest_per_country)\n",
    "\n",
    "multi = (df\n",
    "  .groupBy(\"Country\")\n",
    "  .agg(\n",
    "      count(\"*\").alias(\"Rows\"),\n",
    "      countDistinct(\"CustomerId\").alias(\"DistinctCustomers\"),\n",
    "      min(\"LastUpdatedDate\").alias(\"OldestUpdate\"),\n",
    "      max(\"LastUpdatedDate\").alias(\"NewestUpdate\")\n",
    "  )\n",
    "  .orderBy(col(\"Rows\").desc())\n",
    ")\n",
    "#display(multi)\n",
    "\n",
    "by_month= (df\n",
    "           .groupby(month(col(\"LastUpdatedDate\")))\n",
    "           .agg(count(\"*\").alias(\"Rows\"))\n",
    "           .orderBy(month(col(\"LastUpdatedDate\")))\n",
    ")\n",
    "#display(by_month)\n",
    "\n",
    "task2= (df\n",
    ".groupby(col(\"Country\"))\n",
    ".agg(count(col(\"CustomerId\")).alias(\"CustomerCount\"))\n",
    ".orderBy(col(\"CustomerCount\").desc())\n",
    ")\n",
    "#display(task2)\n",
    "\n",
    "#************Joins\n",
    "orders_data = [\n",
    "    (101, 1, 120.0, \"2025-12-01\"),\n",
    "    (102, 1,  75.0, \"2025-12-02\"),\n",
    "    (103, 2,  50.0, \"2025-12-03\"),\n",
    "    (104, 4, 200.0, \"2025-12-05\"),\n",
    "    (105, 99, 60.0, \"2025-12-06\")  # customerId 99 doesn't exist -> join test\n",
    "]\n",
    "orders = spark.createDataFrame(orders_data, [\"OrderId\", \"CustomerId\", \"Amount\", \"OrderDate\"]) \\\n",
    "             .withColumn(\"OrderDate\", to_date(\"OrderDate\"))\n",
    "#display(orders)\n",
    "\n",
    "\n",
    "inner_join = df.join(orders, on=\"CustomerId\",how=\"inner\")\n",
    "#display(inner_join)\n",
    "\n",
    "left_join = df.join(orders, on=\"CustomerId\",how=\"left\")\n",
    "#display(left_join)  \n",
    "\n",
    "right_join = df.join(orders, on=\"CustomerId\",how=\"right\")\n",
    "#display(right_join)\n",
    "\n",
    "cust_spend = (df\n",
    "  .join(orders, on=\"CustomerId\", how=\"left\")\n",
    "  .groupBy(\"CustomerId\", \"CustomerName\", \"Country\")\n",
    "  .agg(\n",
    "      count(\"OrderId\").alias(\"OrderCount\"),\n",
    "      coalesce(sum(\"Amount\"), lit(0.0)).alias(\"TotalAmount\")\n",
    "  )\n",
    "  .orderBy(col(\"TotalAmount\").desc())\n",
    ")\n",
    "\n",
    "#display(cust_spend)\n",
    "\n",
    "country_order_revenue= (df\n",
    "                        .join(orders,on=\"CustomerId\",how=\"inner\")\n",
    "                        .groupBy(\"Country\")\n",
    "                        .agg(count(\"*\").alias(\"TotalOrders\")\n",
    "                            ,sum(\"Amount\").alias(\"TotalRevenue\"))\n",
    "                        .orderBy(col(\"TotalRevenue\").desc())\n",
    ")\n",
    "display(country_order_revenue)\n",
    "\n",
    "\n",
    "#**************to write the output in another file\n",
    "#spark.sql(\"SHOW VOLUMES IN workspace.default\").show()\n",
    "\n",
    "#out = \"/Volumes/workspace/default/mainvol/deepika/week1/customers_parquet\"\n",
    "#df.write.mode(\"overwrite\").parquet(out)\n",
    "\n",
    "#df_back = spark.read.parquet(out)\n",
    "#display(df_back)\n",
    "#print(df_back.count())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b4d20c7-f386-4553-add3-adcc7b985167",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "W1D1_Read_CSV_Basics",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
