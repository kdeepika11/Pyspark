{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3eafa91f-a141-4a42-aba0-112e57aef766",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS workspace.default.etl_audit_results (\n",
    "  run_id       STRING,\n",
    "  pipeline     STRING,\n",
    "  entity       STRING,\n",
    "  check_name   STRING,\n",
    "  status       STRING,   -- PASS / FAIL\n",
    "  metric_value STRING,   -- store counts/values as string\n",
    "  details      STRING,\n",
    "  created_at   TIMESTAMP\n",
    ")\n",
    "USING DELTA\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "871c76a0-6712-4a3f-adb8-f14c02f59c8c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"run_id\":216},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766597621679}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime\n",
    "\n",
    "run_id= datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "pipeline= \"demo_pipeline\"\n",
    "entity = \"silver_orders\"\n",
    "\n",
    "#print(\"RunId:\", run_id)\n",
    "df=spark.table(\"workspace.default.silver_orders\")\n",
    "#display(df)\n",
    "\n",
    "def log_check(check_name: str, status: str, metric_value: str, details: str = \"\"):\n",
    "    row = [(run_id, pipeline, entity, check_name, status, metric_value, details, datetime.utcnow())]\n",
    "    audit_df = spark.createDataFrame(\n",
    "        row,\n",
    "        [\"run_id\",\"pipeline\",\"entity\",\"check_name\",\"status\",\"metric_value\",\"details\",\"created_at\"]\n",
    "    )\n",
    "    audit_df.write.mode(\"append\").saveAsTable(\"workspace.default.etl_audit_results\")\n",
    "\n",
    "#Row count\n",
    "row_count = df.count()\n",
    "log_check(\"row_count\", \"PASS\", str(row_count), \"Total rows in silver_orders\")\n",
    "print(\"Row count:\", row_count)\n",
    "\n",
    "#display(spark.table(\"workspace.default.etl_audit_results\"))\n",
    "#Null check on key columns (CustomerId, OrderId)\n",
    "null_check = df.filter(F.col(\"CustomerId\").isNull() | F.col(\"OrderId\").isNull()).count()\n",
    "log_check(\"null_check\", \"PASS\" if null_check == 0 else \"FAIL\", str(null_check), \"Null count in silver_orders\")\n",
    "print(\"Null check:\", null_check)\n",
    "\n",
    "#Duplicate check on business key (CustomerId, OrderId)\n",
    "duplicat_check= df.groupby(\"CustomerId\",\"OrderId\").count().filter(F.col(\"count\") > 1).count()\n",
    "log_check(\"duplicate_check\", \"PASS\" if duplicat_check == 0 else \"FAIL\", str(duplicat_check), \"Duplicate count in silver_orders\")\n",
    "print(\"Duplicate check:\", duplicat_check)\n",
    "\n",
    "#Amount should be non-negative\n",
    "amount_check = df.filter(F.col(\"Amount\") < 0).count()\n",
    "log_check(\"amount_check\", \"PASS\" if amount_check == 0 else \"FAIL\", str(amount_check), \"Negative amount count in silver_orders\")\n",
    "print(\"Amount check:\", amount_check)\n",
    "\n",
    "#Freshness check (latest OrderDate not too old)\n",
    "max_order_date = df.agg(F.max(\"OrderDate\").alias(\"max_dt\")).first()[\"max_dt\"]\n",
    "log_check(\"max_order_date\", \"PASS\", str(max_order_date), \"Freshness indicator\")\n",
    "print(\"Max OrderDate:\", max_order_date)\n",
    "\n",
    "audit = spark.table(\"workspace.default.etl_audit_results\")\n",
    "display(audit.filter(F.col(\"run_id\") == run_id).orderBy(\"created_at\"))\n",
    "\n",
    "fails = (spark.table(\"workspace.default.etl_audit_results\")\n",
    "         .filter((F.col(\"run_id\") == run_id) & (F.col(\"status\") == \"FAIL\"))\n",
    "         .count())\n",
    "\n",
    "if fails > 0:\n",
    "    raise Exception(f\"Data Quality checks failed: {fails} checks failed. See etl_audit_results for run_id={run_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "012a8156-669f-41f0-8d30-fc86e20af30c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "W2D2_dataqualitychecks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
