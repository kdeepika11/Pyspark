{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdd02209-4476-404a-8c2e-f9404a298cf9",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"pipeline\":170},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766605951171}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime\n",
    "\n",
    "run_id= datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "pipeline=\"demo_pipeline_audit\"\n",
    "\n",
    "def log_check(entity:str,check_name:str,status:str,metric_value, details:str=\"\"):\n",
    "    row=[(run_id,pipeline,entity,check_name,status,str(metric_value),details,datetime.utcnow())]\n",
    "    audit_df=spark.createDataFrame(row,[\"run_id\",\"pipeline\",\"entity\",\"check_name\",\"status\",\"metric_value\",\"details\",\"check_time\"])\n",
    "    audit_df.write.mode(\"append\").saveAsTable(\"pipeline_audit\")\n",
    "\n",
    "def pass_fail(actual:int , op:str , threshold:int) -> str:\n",
    "    if op==\"==\":\n",
    "        return \"Pass\" if actual == threshold else \"Fail\"\n",
    "    elif op==\"<=\":\n",
    "        return \"Pass\" if actual <= threshold else \"Fail\"\n",
    "    elif op==\">=\":\n",
    "        return \"Pass\" if actual >= threshold else \"Fail\"\n",
    "    elif op==\">\":\n",
    "        return \"Pass\" if actual > threshold else \"Fail\"\n",
    "    elif op==\"<\":\n",
    "        return \"Pass\" if actual < threshold else \"Fail\"\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid operator {op}\")\n",
    "\n",
    "def check_row_count(df,entity:str):\n",
    "    cnt=df.count()\n",
    "    log_check(entity,\"row_count\",\"Pass\",cnt,\"Total Rows\")\n",
    "    return cnt\n",
    "\n",
    "def check_nulls(df,entity:str,cols:list,threshold:int=0,op:str=\"==\"):\n",
    "    cond=None\n",
    "    for col in cols:\n",
    "        expr=F.col(col).isNull()\n",
    "        cond= expr if cond is None else cond | expr\n",
    "    cnt=df.filter(cond).count()\n",
    "    status=pass_fail(cnt,op,threshold)\n",
    "    log_check(entity,f\"null_check({\",\".join(cols)})\",status,cnt,f\"Nulls in {cols}\")\n",
    "    return cnt\n",
    "\n",
    "def check_duplicates(df, entity: str, key_cols: list, threshold: int = 0, op: str = \"==\"):\n",
    "    actual = (df.groupBy(*key_cols).count().filter(F.col(\"count\") > 1).count())\n",
    "    status = pass_fail(actual, op, threshold)\n",
    "    log_check(entity, f\"duplicate_keys({','.join(key_cols)})\", status, actual, f\"Expected {op} {threshold} duplicate groups\")\n",
    "    return actual\n",
    "\n",
    "def check_negative(df, entity: str, colname: str, threshold: int = 0, op: str = \"==\"):\n",
    "    actual = df.filter(F.col(colname) < 0).count()\n",
    "    status = pass_fail(actual, op, threshold)\n",
    "    log_check(entity, f\"negative({colname})\", status, actual, f\"Expected {op} {threshold}\")\n",
    "    return actual\n",
    "\n",
    "\n",
    "silver_orders = spark.table(\"workspace.default.silver_orders\")\n",
    "entity = \"silver_orders\"\n",
    "\n",
    "check_row_count(silver_orders, entity)\n",
    "check_nulls(silver_orders, entity, [\"CustomerId\", \"OrderId\"], threshold=0, op=\"==\")\n",
    "check_duplicates(silver_orders, entity, [\"CustomerId\", \"OrderId\"], threshold=0, op=\"==\")\n",
    "check_negative(silver_orders, entity, \"Amount\", threshold=0, op=\"==\")\n",
    "\n",
    "max_dt = silver_orders.agg(F.max(\"OrderDate\").alias(\"max_dt\")).first()[\"max_dt\"]\n",
    "log_check(entity, \"max_order_date\", \"PASS\", max_dt, \"Freshness indicator\")\n",
    "\n",
    "\n",
    "silver_customers = spark.table(\"workspace.default.silver_customers\")\n",
    "entity = \"silver_customers\"\n",
    "\n",
    "check_row_count(silver_customers, entity)\n",
    "check_nulls(silver_customers, entity, [\"CustomerId\"], threshold=0, op=\"==\")\n",
    "check_duplicates(silver_customers, entity, [\"CustomerId\"], threshold=0, op=\"==\")\n",
    "\n",
    "# threshold example: allow < 5 missing names\n",
    "missing_name = silver_customers.filter(\n",
    "    F.col(\"CustomerName\").isNull() | (F.trim(F.col(\"CustomerName\")) == \"\")\n",
    ").count()\n",
    "status = pass_fail(missing_name, \"<\", 5)\n",
    "log_check(entity, \"missing_customer_name\", status, missing_name, \"Expected < 5\")\n",
    "\n",
    "audit = spark.table(\"workspace.default.pipeline_audit\")\n",
    "display(audit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a7a5a38-e68c-483e-af10-4ecdb0ea4bed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_orders = spark.table(\"workspace.default.bronze_orders\")\n",
    "\n",
    "bronze_distinct_keys = (bronze_orders\n",
    "  .select(\"CustomerId\",\"OrderId\")\n",
    "  .dropna()\n",
    "  .dropDuplicates()\n",
    "  .count()\n",
    ")\n",
    "\n",
    "silver_cnt = spark.table(\"workspace.default.silver_orders\").count()\n",
    "\n",
    "status = \"PASS\" if silver_cnt == bronze_distinct_keys else \"FAIL\"\n",
    "log_check(\n",
    "    \"recon_orders\",\n",
    "    \"silver_count_equals_bronze_distinct_keys\",\n",
    "    status,\n",
    "    f\"silver={silver_cnt}, bronze_distinct_keys={bronze_distinct_keys}\",\n",
    "    \"Expected equality (deduped business keys)\"\n",
    ")\n",
    "\n",
    "bronze_customers = spark.table(\"workspace.default.bronze_customers\")\n",
    "\n",
    "bronze_distinct_cust = (bronze_customers\n",
    "  .select(\"CustomerId\")\n",
    "  .dropna()\n",
    "  .dropDuplicates()\n",
    "  .count()\n",
    ")\n",
    "\n",
    "silver_cust_cnt = spark.table(\"workspace.default.silver_customers\").count()\n",
    "\n",
    "status = \"PASS\" if silver_cust_cnt == bronze_distinct_cust else \"FAIL\"\n",
    "log_check(\n",
    "    \"recon_customers\",\n",
    "    \"silver_count_equals_bronze_distinct_customerid\",\n",
    "    status,\n",
    "    f\"silver={silver_cust_cnt}, bronze_distinct_customerid={bronze_distinct_cust}\",\n",
    "    \"Expected equality (deduped CustomerId)\"\n",
    ")\n",
    "\n",
    "audit = spark.table(\"workspace.default.etl_audit_results\")\n",
    "display(audit.filter(F.col(\"run_id\") == run_id).orderBy(\"created_at\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ReusableAuditLog",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
