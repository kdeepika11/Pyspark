{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfd14e3d-5369-4d10-aed2-42d9e2065768",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "#******bronze prepare table***********#\n",
    "customers_src=spark.table(\"workspace.default.customers\")\n",
    "orders_hist_data = [\n",
    "    (1, 101, 120.0, \"2025-12-01\"),\n",
    "    (1, 102,  75.0, \"2025-12-02\"),\n",
    "    (1, 107, 130.0, \"2025-12-08\"),\n",
    "    (2, 103,  50.0, \"2025-12-03\"),\n",
    "    (2, 108,  50.0, \"2025-12-10\"),\n",
    "    (4, 104, 200.0, \"2025-12-05\"),\n",
    "    (4, 106, 210.0, \"2025-12-07\")\n",
    "]\n",
    "orders_src = (spark.createDataFrame(orders_hist_data, [\"CustomerId\",\"OrderId\",\"Amount\",\"OrderDate\"])\n",
    "              .withColumn(\"OrderDate\", F.to_date(\"OrderDate\")))\n",
    "\n",
    "customers_bronze = (customers_src\n",
    "    .withColumn(\"_ingested_at\", F.current_timestamp())\n",
    "    .withColumn(\"_source\", F.lit(\"demo_customers\"))\n",
    ")\n",
    "\n",
    "orders_bronze = (orders_src\n",
    "    .withColumn(\"_ingested_at\", F.current_timestamp())\n",
    "    .withColumn(\"_source\", F.lit(\"demo_orders\"))\n",
    ")\n",
    "\n",
    "customers_bronze.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"workspace.default.bronze_customers\")\n",
    "orders_bronze.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"workspace.default.bronze_orders\")\n",
    "display(spark.table(\"workspace.default.bronze_customers\"))\n",
    "display(spark.table(\"workspace.default.bronze_orders\"))\n",
    "\n",
    "#******bronze prepare table***********#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcd49afb-4658-4a24-b938-2c0f317284e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "#******silver table clean\\dedupe***********#\n",
    "bronze_customers = spark.table(\"workspace.default.bronze_customers\")\n",
    "silver_customers =(bronze_customers\n",
    ".withColumn(\"CustomerName\", F.trim(F.col(\"CustomerName\")))\n",
    ".withColumn(\"Country\", F.upper(F.trim(F.col(\"Country\"))))\n",
    ".withColumn(\"CustomerId\",F.col(\"CustomerId\").cast(\"long\"))\n",
    ".withColumn(\"LastUpdatedDate\",F.col(\"LastUpdatedDate\").cast(\"timestamp\"))\n",
    ")\n",
    "silver_customers.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"workspace.default.silver_customers\")\n",
    "display(bronze_customers)\n",
    "display(spark.table(\"workspace.default.silver_customers\"))\n",
    "\n",
    "#**Keep latest\n",
    "bronze_orders = spark.table(\"workspace.default.bronze_orders\")\n",
    "\n",
    "w=Window.partitionBy(\"CustomerId\",\"OrderId\").orderBy(F.col(\"_ingested_at\").desc())\n",
    "\n",
    "\n",
    "silver_orders =(bronze_orders\n",
    ".withColumn(\"OrderId\",F.col(\"OrderId\").cast(\"long\"))\n",
    ".withColumn(\"CustomerId\",F.col(\"CustomerId\").cast(\"long\"))\n",
    ".withColumn(\"Amount\",F.col(\"Amount\").cast(\"double\"))\n",
    ".withColumn(\"OrderDate\",F.col(\"OrderDate\").cast(\"date\"))\n",
    ".withColumn(\"rn\",F.row_number().over(w))\n",
    ".filter(F.col(\"rn\")==1)\n",
    ".drop(\"rn\")\n",
    ")\n",
    "silver_orders.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"workspace.default.silver_orders\")\n",
    "display(bronze_orders)\n",
    "display(spark.table(\"workspace.default.silver_orders\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac87ab64-6f07-4ae1-9a55-d59bfa773eb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#******Gold aggreagates for reporting***********#\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "silver_customers = spark.table(\"workspace.default.silver_customers\")\n",
    "silver_orders = spark.table(\"workspace.default.silver_orders\")\n",
    "\n",
    "gold_revenue =(silver_orders\n",
    ".join(silver_customers,on=\"CustomerId\",how=\"left\")\n",
    ".groupBy(\"Country\")\n",
    ".agg(\n",
    "    F.count(\"*\").alias(\"TotalOrders\"),\n",
    "    F.sum(\"Amount\").alias(\"TotalRevenue\"),\n",
    "    F.round(F.avg(\"Amount\"),2).alias(\"AvgOrderAmount\")\n",
    "    )\n",
    ".orderBy(F.col(\"TotalRevenue\").desc())\n",
    ")\n",
    "gold_revenue.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"workspace.default.gold_revenue\")\n",
    "display(gold_revenue)\n",
    "\n",
    "\n",
    "w_latest = Window.partitionBy(\"CustomerId\").orderBy(F.col(\"OrderDate\").desc(), F.col(\"OrderId\").desc())\n",
    "\n",
    "gold_latest_order_per_customer = (silver_orders\n",
    "    .withColumn(\"rn\", F.row_number().over(w_latest))\n",
    "    .filter(F.col(\"rn\") == 1)\n",
    "    .drop(\"rn\")\n",
    "    .join(silver_customers.select(\"CustomerId\",\"CustomerName\",\"Country\"), \"CustomerId\", \"left\")\n",
    "    .select(\"CustomerId\",\"CustomerName\",\"Country\",\"OrderId\",\"Amount\",\"OrderDate\")\n",
    ")\n",
    "\n",
    "gold_latest_order_per_customer.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(\"workspace.default.gold_latest_order_per_customer\")\n",
    "display(spark.table(\"workspace.default.gold_latest_order_per_customer\"))\n",
    "\n",
    "w=Window.partitionBy(F.date_format(F.col(\"OrderDate\"),\"yyyy-MM\")).orderBy(F.col(\"OrderDate\").desc(), F.col(\"OrderId\").desc())\n",
    "gold_monthly_revenue=(silver_orders\n",
    "                      .withColumn(\"Month\",F.date_format(F.col(\"OrderDate\"),\"yyyy-MM\"))\n",
    "                      .withColumn(\"rn\", F.row_number().over(w))\n",
    "                      .filter(F.col(\"rn\") == 1)\n",
    "                      .drop(\"rn\")\n",
    "                      .groupBy(\"Month\")\n",
    "                      .agg(F.sum(\"Amount\").alias(\"TotalRevenue\"))\n",
    "                      .orderBy(F.col(\"Month\").asc())\n",
    "                      )\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS workspace.default.gold_monthly_revenue\")                    \n",
    "gold_monthly_revenue.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(\"workspace.default.gold_monthly_revenue\")\n",
    "display(spark.table(\"workspace.default.gold_monthly_revenue\"))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "W2D2_mini_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
